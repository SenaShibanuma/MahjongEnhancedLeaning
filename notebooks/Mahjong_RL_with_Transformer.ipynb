{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 麻雀 Transformer AI 強化学習 (PPO + Custom Policy)\n",
    "\n",
    "**目的:** 事前学習済みの Transformer モデルを基盤として、Stable Baselines3 の PPO アルゴリズムを用い、カスタム麻雀環境 (`MahjongEnv`) で自己対戦学習を実行します。\n",
    "\n",
    "このノートブックは、Google Driveに配置されたPythonモジュールを呼び出し、学習を実行するためのランチャーです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ1: 環境準備\n",
    "\n",
    "### GPUの有効化\n",
    "メニューから **「ランタイム」 > 「ランタイムのタイプを変更」** を選択し、**「ハードウェアアクセラレータ」** で **「GPU」** を選択してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libs"
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install mahjong gymnasium \"stable-baselines3[extra]\" torch numpy\n",
    "\n",
    "import torch\n",
    "\n",
    "# GPUの利用状況を確認\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"✅ GPUが利用可能です: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"❌ GPUが利用できません。CPUを使用します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ2: Google Driveのマウントとプロジェクトパス設定\n",
    "\n",
    "自作のPythonモジュール (`MahjongEnv`, `MaskedTransformerPolicy` など) をインポートするために、Google Driveをマウントし、プロジェクトのルートディレクトリをPythonのパスに追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Google Driveをマウント\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# 【重要】ご自身の環境に合わせてプロジェクトのルートディレクトリを設定してください\n",
    "PROJECT_ROOT = '/content/gdrive/MyDrive/MahjongEnhancedLeaning' # 例: '/content/gdrive/MyDrive/YourProjectFolder'\n",
    "\n",
    "# 作業ディレクトリを設定し、パスに追加\n",
    "if not os.path.exists(PROJECT_ROOT):\n",
    "    raise FileNotFoundError(f\"指定されたプロジェクトパスが見つかりません: {PROJECT_ROOT}\")\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(f\"作業ディレクトリ: {os.getcwd()}\")\n",
    "print(f\"Pythonパスに '{PROJECT_ROOT}' を追加しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ3: モジュールのインポートと強化学習のセットアップ\n",
    "\n",
    "Google Driveからカスタムモジュールをインポートし、強化学習のハイパーパラメータを設定してPPOモデルを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_rl"
   },
   "outputs": [],
   "source": [
    "# --- 必要なモジュールをインポート ---\n",
    "from stable_baselines3 import PPO\n",
    "from mahjong_rl_env.environment import MahjongEnv\n",
    "from models.custom_policy import MaskedTransformerPolicy\n",
    "\n",
    "print(\"カスタムモジュールのインポートに成功しました。\")\n",
    "\n",
    "# --- ハイパーパラメータ設定 ---\n",
    "RL_AGENT_ID = 0\n",
    "FEATURES_DIM = 512\n",
    "LEARNING_RATE = 3e-5\n",
    "TOTAL_TIMESTEPS = 500_000  # 総学習ステップ数 (必要に応じて調整)\n",
    "N_STEPS = 2048           # 1回の経験収集ステップ数\n",
    "\n",
    "# --- 保存パスの設定 ---\n",
    "TENSORBOARD_LOG_PATH = os.path.join(PROJECT_ROOT, \"tensorboard_logs/\")\n",
    "MODEL_SAVE_PATH = os.path.join(PROJECT_ROOT, \"models\", \"mahjong_ppo_agent.zip\")\n",
    "\n",
    "# 1. 環境のインスタンス化\n",
    "env = MahjongEnv(agent_id=RL_AGENT_ID)\n",
    "print(f\"MahjongEnvを作成しました。RLエージェントはプレイヤー {RL_AGENT_ID}です。\")\n",
    "\n",
    "# 2. カスタムポリシーのための引数設定\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_kwargs=dict(\n",
    "        features_dim=FEATURES_DIM\n",
    "        # ここでTransformerFeatureExtractorに渡す引数を追加可能\n",
    "    ),\n",
    "    # Actor/Critic の MLP 層の定義\n",
    "    net_arch=[dict(pi=[256, 256], vf=[256, 256])]\n",
    ")\n",
    "\n",
    "# 3. PPOモデルの初期化\n",
    "# 事前学習済み重みは、MaskedTransformerPolicy内のTransformerFeatureExtractorで自動的にロードされます。\n",
    "model = PPO(\n",
    "    MaskedTransformerPolicy,\n",
    "    env,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_steps=N_STEPS,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    verbose=1,\n",
    "    tensorboard_log=TENSORBOARD_LOG_PATH,\n",
    "    device=device,\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "print(\"PPOモデルとカスタムポリシーを初期化しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ4: 強化学習の実行\n",
    "\n",
    "設定した内容で強化学習を開始します。学習の進捗はTensorBoardで確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "print(\"--- 強化学習を開始します ---\")\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, progress_bar=True)\n",
    "print(\"--- 学習が完了しました ---\")\n",
    "\n",
    "# 学習済みモデルの保存\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"学習済みモデルを '{MODEL_SAVE_PATH}' に保存しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ5: 学習済みモデルの評価\n",
    "\n",
    "保存したモデルをロードし、エージェントがどのように行動するかをテストします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    # モデルをロード\n",
    "    loaded_model = PPO.load(MODEL_SAVE_PATH, env=env)\n",
    "    print(\"モデルをロードしました。評価を開始します。\")\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    total_reward = 0\n",
    "    turn_count = 0\n",
    "\n",
    "    while not terminated:\n",
    "        turn_count += 1\n",
    "        print(f\"\\n===== ターン {turn_count} =====\")\n",
    "        env.render()\n",
    "        \n",
    "        action, _states = loaded_model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        print(f\"AIが行動 {action} を選択しました。 報酬: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(\"\\n--- 局が終了しました ---\")\n",
    "            print(f\"最終スコア: {env.game_state['scores']}\")\n",
    "            print(f\"この局の合計報酬: {total_reward}\")\n",
    "            break\n",
    "else:\n",
    "    print(f\"保存されたモデルが見つかりません: {MODEL_SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
